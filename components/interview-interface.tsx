"use client"

import { useState, useEffect, useRef } from "react"
import { Button } from "@/components/ui/button"
import { Card, CardContent } from "@/components/ui/card"
import { Textarea } from "@/components/ui/textarea"
import { Avatar, AvatarFallback } from "@/components/ui/avatar"
import { Progress } from "@/components/ui/progress"
import { Mic, MicOff, Send, Volume2, VolumeX, Loader2, CheckCircle, ArrowRight } from 'lucide-react'
import Link from "next/link"
import Image from "next/image"
import { useRouter } from 'next/navigation'
import { Badge } from "@/components/ui/badge"
import { GeminiLiveSDK } from "@/lib/gemini-live-sdk"
import { InterviewContext } from "@/lib/gemini"
import { AudioRecorder, transcribeAudio, playTextToSpeech } from "@/lib/recordAudio"

type Message = {
  role: "ai" | "user"
  content: string
  timestamp: Date
}

type InterviewPhase = "intro" | "technical" | "behavioral" | "closing"

export function InterviewInterface({ jobId }: { jobId: string }) {
  const router = useRouter()
  const [messages, setMessages] = useState<Message[]>([])
  const [currentInput, setCurrentInput] = useState("")
  const [isRecording, setIsRecording] = useState(false)
  const [isAISpeaking, setIsAISpeaking] = useState(false)
  const [isLoading, setIsLoading] = useState(false)
  const [interviewPhase, setInterviewPhase] = useState<InterviewPhase>("intro")
  const [questionCount, setQuestionCount] = useState(0)
  const [interviewComplete, setInterviewComplete] = useState(false)
  const [isConnected, setIsConnected] = useState(false)
  const [interviewType, setInterviewType] = useState<'personality' | 'technical'>('personality')
  const [speakerEnabled, setSpeakerEnabled] = useState(false) // TTS toggle
  const messagesEndRef = useRef<HTMLDivElement>(null)
  const hasInitialized = useRef(false)
  const geminiClientRef = useRef<GeminiLiveSDK | null>(null)
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioContextRef = useRef<AudioContext | null>(null)
  const mediaStreamRef = useRef<MediaStream | null>(null)
  const audioProcessorRef = useRef<ScriptProcessorNode | null>(null)
  const audioRecorderRef = useRef<AudioRecorder | null>(null) // New audio recorder

  const totalQuestions = 8
  const progress = (questionCount / totalQuestions) * 100

  // Initialize Gemini Live connection
  useEffect(() => {
    console.log('üé¨ Starting interview initialization for job ID:', jobId)
    
    const initializeGemini = async () => {
      try {
        console.log('üì° Fetching job data for ID:', jobId)
        // Fetch job data and user profile
        const jobResponse = await fetch(`/api/jobs/${jobId}`)
        
        if (!jobResponse.ok) {
          throw new Error(`Failed to fetch job: ${jobResponse.status} ${jobResponse.statusText}`)
        }
        
        const jobData = await jobResponse.json()
        console.log('‚úÖ Job data loaded:')
        console.log('  - Title:', jobData.title)
        console.log('  - Company:', jobData.company)
        console.log('  - Skills:', jobData.skills?.length || 0)
        console.log('  - Responsibilities:', jobData.responsibilities?.length || 0)
        
        // Get user profile from localStorage (set during onboarding)
        const userProfile = JSON.parse(localStorage.getItem('userProfile') || '{}')
        console.log('‚úÖ User profile loaded:')
        console.log('  - Name:', userProfile.fullName || 'Not set')
        console.log('  - Experience:', userProfile.experience || '0', 'years')
        console.log('  - Skills:', userProfile.skills || 'Not set')
        console.log('  - Resume:', userProfile.resumeFileName || 'Not uploaded')
        
        // Create interview context
        const context: InterviewContext = {
          jobTitle: jobData.title || 'Software Engineer',
          company: jobData.company || 'Company',
          jobDescription: jobData.description || '',
          responsibilities: jobData.responsibilities || [],
          requiredQualifications: jobData.required || [],
          preferredQualifications: jobData.preferred || [],
          skills: jobData.skills || [],
          userProfile: {
            name: userProfile.fullName || 'Candidate',
            email: userProfile.email || '',
            age: userProfile.age,
            nationality: userProfile.nationality || '',
            skills: userProfile.skills || '',
            experience: userProfile.experience || '0',
            resumeText: userProfile.resumeText
          }
        }

        // Get API key from environment
        const apiKey = process.env.NEXT_PUBLIC_GEMINI_API_KEY || ''
        
        console.log('üîë API Key check:', apiKey ? 'Found ‚úÖ' : 'Missing ‚ùå')
        console.log('üìÑ Job data:', jobData.title)
        console.log('üë§ User profile:', userProfile.fullName || 'Not set')
        
        if (!apiKey) {
          console.error('‚ùå Gemini API key not found')
          setMessages([{
            role: "ai",
            content: "Error: API key not configured. Please add your Gemini API key to continue.",
            timestamp: new Date()
          }])
          return
        }

        console.log('üí¨ Using text-based interview mode')
        
        // Store context for text-based API
        sessionStorage.setItem('interviewContext', JSON.stringify(context))
        setIsConnected(true)
        
        // Send initial greeting based on interview type
        const getInitialMessage = () => {
          if (interviewType === 'technical') {
            return "ÏïàÎÖïÌïòÏÑ∏Ïöî! Í∏∞Ïà† Î©¥Ï†ë Ïó∞Ïäµ ÏÑ∏ÏÖòÏóê Ïò§Ïã† Í≤ÉÏùÑ ÌôòÏòÅÌï©ÎãàÎã§. Ïò§ÎäòÏùÄ ÏßÄÏõêÏûêÎãòÏùò Í∏∞Ïà†Ï†Å Ïó≠ÎüâÍ≥º Î¨∏Ï†ú Ìï¥Í≤∞ Îä•Î†•ÏùÑ ÌèâÍ∞ÄÌïòÍ≤†ÏäµÎãàÎã§.\n\nÏßÄÏõêÏûêÎãòÏùò ÌîÑÎ°úÌïÑÍ≥º Ï±ÑÏö© Í≥µÍ≥†Î•º Í≤ÄÌÜ†ÌñàÏäµÎãàÎã§. Î∞îÎ°ú ÏãúÏûëÌïòÍ≤†ÏäµÎãàÎã§.\n\nÏµúÍ∑ºÏóê ÏûëÏóÖÌïòÏã† Í∏∞Ïà†Ï†ÅÏúºÎ°ú Í∞ÄÏû• Î≥µÏû°ÌñàÎçò ÌîÑÎ°úÏ†ùÌä∏Ïóê ÎåÄÌï¥ ÎßêÏîÄÌï¥ Ï£ºÏÑ∏Ïöî. Ïñ¥Îñ§ Í∏∞Ïà† Ïä§ÌÉùÏùÑ ÏÇ¨Ïö©ÌïòÏÖ®Í≥†, Ïñ¥Îñ§ Í∏∞Ïà†Ï†Å Ï±åÎ¶∞ÏßÄÍ∞Ä ÏûàÏóàÏúºÎ©∞, Ïñ¥ÎñªÍ≤å Ìï¥Í≤∞ÌïòÏÖ®ÎÇòÏöî?"
          } else {
            return "ÏïàÎÖïÌïòÏÑ∏Ïöî! Ïù∏ÏÑ± Î©¥Ï†ë Ïó∞Ïäµ ÏÑ∏ÏÖòÏóê Ïò§Ïã† Í≤ÉÏùÑ ÌôòÏòÅÌï©ÎãàÎã§. Ïò§ÎäòÏùÄ ÏßÄÏõêÏûêÎãòÏùò ÌñâÎèô ÌäπÏÑ±, ÌåÄÏõåÌÅ¨ Îä•Î†•, Í∑∏Î¶¨Í≥† Î¨∏Ìôî Ï†ÅÌï©ÏÑ±ÏùÑ ÌèâÍ∞ÄÌïòÍ≤†ÏäµÎãàÎã§.\n\nÏßÄÏõêÏûêÎãòÏùò ÌîÑÎ°úÌïÑÍ≥º Î∞∞Í≤ΩÏùÑ Í≤ÄÌÜ†ÌñàÏäµÎãàÎã§. Î®ºÏ†Ä Í∞ÑÎã®Ìïú ÏûêÍ∏∞ÏÜåÍ∞úÎ°ú ÏãúÏûëÌïòÍ≤†ÏäµÎãàÎã§.\n\nÎ≥∏Ïù∏Ïóê ÎåÄÌï¥, Ïª§Î¶¨Ïñ¥ Ïó¨Ï†ï, Í∞ïÏ†ê, Í∑∏Î¶¨Í≥† Ïù¥ Ìè¨ÏßÄÏÖòÏóê Í¥ÄÏã¨ÏùÑ Í∞ÄÏßÄÍ≤å Îêú Ïù¥Ïú†Î•º ÎßêÏîÄÌï¥ Ï£ºÏÑ∏Ïöî."
          }
        }
        
        if (!hasInitialized.current) {
          setMessages([{
            role: "ai",
            content: getInitialMessage(),
            timestamp: new Date()
          }])
          hasInitialized.current = true
        }
        
        console.log('‚úÖ Text interview ready!')
      } catch (error) {
        console.error('‚ùå Failed to initialize Gemini:', error)
        console.error('Error details:', error)
        
        // Fallback to initial message and enable text API mode
        setMessages([{
          role: "ai",
          content: "Hello! Welcome to your interview practice session. I'm your AI interviewer today. Let's start with a brief introduction. Could you tell me about yourself and why you're interested in this position?",
          timestamp: new Date()
        }])
        
        // Store context for text-based API fallback
        const userProfile = JSON.parse(localStorage.getItem('userProfile') || '{}')
        const context: InterviewContext = {
          jobTitle: 'Interview Position',
          company: 'Company',
          jobDescription: '',
          responsibilities: [],
          requiredQualifications: [],
          preferredQualifications: [],
          skills: [],
          userProfile: {
            name: userProfile.fullName || 'Candidate',
            email: userProfile.email || '',
            age: userProfile.age,
            nationality: userProfile.nationality || '',
            skills: userProfile.skills || '',
            experience: userProfile.experience || '0',
            resumeText: userProfile.resumeText
          }
        }
        sessionStorage.setItem('interviewContext', JSON.stringify(context))
        setIsConnected(true) // Enable text-based API fallback
        console.log('‚úÖ Fallback mode enabled - using Text API')
      }
    }

    initializeGemini()

    return () => {
      // Cleanup on unmount
      if (geminiClientRef.current) {
        geminiClientRef.current.disconnect()
      }
    }
  }, [jobId])

  useEffect(() => {
    messagesEndRef.current?.scrollIntoView({ behavior: "smooth" })
  }, [messages])

  // Update initial message when interview type changes
  useEffect(() => {
    if (messages.length === 1 && hasInitialized.current) {
      const getInitialMessage = () => {
        if (interviewType === 'technical') {
          return "ÏïàÎÖïÌïòÏÑ∏Ïöî! Í∏∞Ïà† Î©¥Ï†ë Ïó∞Ïäµ ÏÑ∏ÏÖòÏóê Ïò§Ïã† Í≤ÉÏùÑ ÌôòÏòÅÌï©ÎãàÎã§. Ïò§ÎäòÏùÄ ÏßÄÏõêÏûêÎãòÏùò Í∏∞Ïà†Ï†Å Ïó≠ÎüâÍ≥º Î¨∏Ï†ú Ìï¥Í≤∞ Îä•Î†•ÏùÑ ÌèâÍ∞ÄÌïòÍ≤†ÏäµÎãàÎã§.\n\nÏßÄÏõêÏûêÎãòÏùò ÌîÑÎ°úÌïÑÍ≥º Ï±ÑÏö© Í≥µÍ≥†Î•º Í≤ÄÌÜ†ÌñàÏäµÎãàÎã§. Î∞îÎ°ú ÏãúÏûëÌïòÍ≤†ÏäµÎãàÎã§.\n\nÏµúÍ∑ºÏóê ÏûëÏóÖÌïòÏã† Í∏∞Ïà†Ï†ÅÏúºÎ°ú Í∞ÄÏû• Î≥µÏû°ÌñàÎçò ÌîÑÎ°úÏ†ùÌä∏Ïóê ÎåÄÌï¥ ÎßêÏîÄÌï¥ Ï£ºÏÑ∏Ïöî. Ïñ¥Îñ§ Í∏∞Ïà† Ïä§ÌÉùÏùÑ ÏÇ¨Ïö©ÌïòÏÖ®Í≥†, Ïñ¥Îñ§ Í∏∞Ïà†Ï†Å Ï±åÎ¶∞ÏßÄÍ∞Ä ÏûàÏóàÏúºÎ©∞, Ïñ¥ÎñªÍ≤å Ìï¥Í≤∞ÌïòÏÖ®ÎÇòÏöî?"
        } else {
          return "ÏïàÎÖïÌïòÏÑ∏Ïöî! Ïù∏ÏÑ± Î©¥Ï†ë Ïó∞Ïäµ ÏÑ∏ÏÖòÏóê Ïò§Ïã† Í≤ÉÏùÑ ÌôòÏòÅÌï©ÎãàÎã§. Ïò§ÎäòÏùÄ ÏßÄÏõêÏûêÎãòÏùò ÌñâÎèô ÌäπÏÑ±, ÌåÄÏõåÌÅ¨ Îä•Î†•, Í∑∏Î¶¨Í≥† Î¨∏Ìôî Ï†ÅÌï©ÏÑ±ÏùÑ ÌèâÍ∞ÄÌïòÍ≤†ÏäµÎãàÎã§.\n\nÏßÄÏõêÏûêÎãòÏùò ÌîÑÎ°úÌïÑÍ≥º Î∞∞Í≤ΩÏùÑ Í≤ÄÌÜ†ÌñàÏäµÎãàÎã§. Î®ºÏ†Ä Í∞ÑÎã®Ìïú ÏûêÍ∏∞ÏÜåÍ∞úÎ°ú ÏãúÏûëÌïòÍ≤†ÏäµÎãàÎã§.\n\nÎ≥∏Ïù∏Ïóê ÎåÄÌï¥, Ïª§Î¶¨Ïñ¥ Ïó¨Ï†ï, Í∞ïÏ†ê, Í∑∏Î¶¨Í≥† Ïù¥ Ìè¨ÏßÄÏÖòÏóê Í¥ÄÏã¨ÏùÑ Í∞ÄÏßÄÍ≤å Îêú Ïù¥Ïú†Î•º ÎßêÏîÄÌï¥ Ï£ºÏÑ∏Ïöî."
        }
      }

      setMessages([{
        role: "ai",
        content: getInitialMessage(),
        timestamp: new Date()
      }])
    }
  }, [interviewType])

  const handleSendMessage = async (messageTextOverride?: string) => {
    const messageText = messageTextOverride || currentInput
    
    if (!messageText.trim() || isLoading) return

    const userMessage: Message = {
      role: "user",
      content: messageText,
      timestamp: new Date()
    }

    setMessages((prev) => [...prev, userMessage])
    setCurrentInput("")
    setIsLoading(true)

    try {
      // Use text-based API
      if (isConnected) {
        // Fallback to text-based API
        console.log('üì§ Sending via Text API fallback')
        
        const storedContext = JSON.parse(sessionStorage.getItem('interviewContext') || '{}')
        
        // Ensure context has required arrays with defaults
        const context: InterviewContext = {
          jobTitle: storedContext.jobTitle || 'Position',
          company: storedContext.company || 'Company',
          jobDescription: storedContext.jobDescription || '',
          responsibilities: Array.isArray(storedContext.responsibilities) ? storedContext.responsibilities : [],
          requiredQualifications: Array.isArray(storedContext.requiredQualifications) ? storedContext.requiredQualifications : [],
          preferredQualifications: Array.isArray(storedContext.preferredQualifications) ? storedContext.preferredQualifications : [],
          skills: Array.isArray(storedContext.skills) ? storedContext.skills : [],
          userProfile: storedContext.userProfile || {
            name: 'Candidate',
            email: '',
            nationality: '',
            skills: '',
            experience: '0'
          }
        }
        
        console.log('üìã Context prepared:', {
          job: context.jobTitle,
          skills: context.skills.length,
          responsibilities: context.responsibilities.length
        })
        
        // Get job data for this specific interview
        const jobResponse = await fetch(`/api/jobs/${jobId}`)
        const jobData = await jobResponse.json()
        
        // Get user profile
        const userProfile = JSON.parse(localStorage.getItem('userProfile') || '{}')
        
        // Convert messages to OpenAI format (exclude initial AI greeting)
        const conversationHistory = messages
          .slice(1) // Skip the first AI greeting
          .map(msg => ({
            role: msg.role === 'user' ? 'user' : 'assistant',
            content: msg.content
          }))

        console.log('üìã Sending context:', {
          job: jobData.title,
          company: jobData.company,
          userSkills: userProfile.skills,
          resumeLength: userProfile.resumeText?.length || 0
        })

        const response = await fetch('/api/interview/openai-chat', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            interviewType,
            jobData: {
              title: jobData.title,
              company: jobData.company,
              description: jobData.description,
              skills: jobData.skills,
              responsibilities: jobData.responsibilities
            },
            userProfile: {
              fullName: userProfile.fullName,
              experience: userProfile.experience,
              skills: userProfile.skills,
              targetRole: userProfile.targetRole,
              resumeText: userProfile.resumeText
            },
            conversationHistory,
            userMessage: messageText
          })
        })

        if (!response.ok) {
          const errorText = await response.text()
          console.error('‚ùå API Error:', response.status, errorText)
          throw new Error(`API request failed: ${response.status} - ${errorText}`)
        }

        const data = await response.json()
        console.log('‚úÖ API response received:', data.message?.substring(0, 50) + '...')
        
        const aiMessage: Message = {
          role: "ai",
          content: data.message,
          timestamp: new Date()
        }
        
        setMessages((prev) => [...prev, aiMessage])
        setIsLoading(false)
        
        // Play TTS if speaker is enabled
        if (speakerEnabled && data.message) {
          try {
            setIsAISpeaking(true)
            await playTextToSpeech(data.message)
            setIsAISpeaking(false)
          } catch (error) {
            console.error('‚ùå TTS playback failed:', error)
            setIsAISpeaking(false)
          }
        }
        
        const newQuestionCount = questionCount + 1
        setQuestionCount(newQuestionCount)
        
        // Update phases
        if (newQuestionCount === 3) setInterviewPhase("technical")
        else if (newQuestionCount === 5) setInterviewPhase("behavioral")
        else if (newQuestionCount === 7) setInterviewPhase("closing")
        
        if (newQuestionCount >= totalQuestions) {
          setInterviewComplete(true)
        }
      } else {
        // No connection at all - should not happen but handle gracefully
        console.error('‚ö†Ô∏è No connection available - using emergency fallback')
        await new Promise((resolve) => setTimeout(resolve, 1000))
        const fallbackMessage: Message = {
          role: "ai",
          content: "Thank you for sharing. Could you tell me more about your experience with that?",
          timestamp: new Date()
        }
        setMessages((prev) => [...prev, fallbackMessage])
        setIsLoading(false)
        setQuestionCount(prev => prev + 1)
      }
    } catch (error) {
      console.error('‚ùå Error sending message:', error)
      // Ultimate fallback
      await new Promise((resolve) => setTimeout(resolve, 1000))
      const fallbackMessage: Message = {
        role: "ai",
        content: "Thank you for sharing. Could you tell me more about your experience with that?",
        timestamp: new Date()
      }
      setMessages((prev) => [...prev, fallbackMessage])
      setIsLoading(false)
      setQuestionCount(prev => prev + 1)
    }
  }

  const handleKeyPress = (e: React.KeyboardEvent) => {
    if (e.key === "Enter" && !e.shiftKey) {
      e.preventDefault()
      handleSendMessage()
    }
  }

  const toggleRecording = async () => {
    if (isRecording) {
      // Stop recording and transcribe
      await stopRecording()
    } else {
      // Start recording
      await startRecording()
    }
  }

  const startRecording = async () => {
    try {
      console.log('üé§ Starting voice recording...')
      
      // Create new audio recorder
      const recorder = new AudioRecorder()
      audioRecorderRef.current = recorder
      
      await recorder.startRecording()
      setIsRecording(true)
      
      console.log('‚úÖ Recording started')
    } catch (error) {
      console.error('‚ùå Failed to start recording:', error)
      alert('Microphone access is required for voice input. Please grant permission and try again.')
      setIsRecording(false)
    }
  }

  const stopRecording = async () => {
    try {
      console.log('üõë Stopping recording...')
      
      if (!audioRecorderRef.current) {
        console.error('‚ùå No active recorder')
        return
      }

      setIsRecording(false)
      setIsLoading(true)

      // Stop recording and get audio blob
      const audioBlob = await audioRecorderRef.current.stopRecording()
      console.log('‚úÖ Recording stopped, blob size:', audioBlob.size, 'bytes')

      // Transcribe audio to text
      console.log('üîÑ Transcribing audio...')
      const transcribedText = await transcribeAudio(audioBlob)
      console.log('‚úÖ Transcription:', transcribedText)

      // Send transcribed text as message
      await handleSendMessage(transcribedText)
      
      // Cleanup
      audioRecorderRef.current = null
    } catch (error) {
      console.error('‚ùå Failed to process recording:', error)
      setIsRecording(false)
      setIsLoading(false)
      alert('Failed to transcribe audio. Please try again.')
    }
  }

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      // Cleanup audio recorder if active
      if (audioRecorderRef.current && audioRecorderRef.current.isRecording()) {
        audioRecorderRef.current.stopRecording().catch(console.error)
      }
      // Cleanup Gemini connection
      if (geminiClientRef.current) {
        geminiClientRef.current.disconnect()
      }
    }
  }, [])

  const getPhaseLabel = () => {
    switch (interviewPhase) {
      case "intro": return "Introduction"
      case "technical": return "Technical Questions"
      case "behavioral": return "Behavioral Questions"
      case "closing": return "Closing"
    }
  }

  const getPhaseColor = () => {
    switch (interviewPhase) {
      case "intro": return "bg-blue-500"
      case "technical": return "bg-purple-500"
      case "behavioral": return "bg-green-500"
      case "closing": return "bg-orange-500"
    }
  }

  return (
    <div className="min-h-screen bg-background flex flex-col">
      {/* Header */}
      <header className="border-b border-border bg-card/50 backdrop-blur-sm">
        <div className="container mx-auto px-4 py-4">
          <div className="flex items-center justify-between">
            <Link href="/dashboard" className="flex items-center gap-2">
              <Image
                src="/logo.jpg"
                alt="IpsaGo Logo"
                width={32}
                height={32}
                className="rounded-lg"
              />
              <span className="font-bold text-xl">IpsaGo</span>
            </Link>
            <div className="flex items-center gap-4">
              <Badge variant="secondary" className="hidden sm:flex">
                {getPhaseLabel()}
              </Badge>
              <div className="text-sm text-muted-foreground">
                Question {questionCount} / {totalQuestions}
              </div>
            </div>
          </div>
          
          {/* Interview Type Navigation */}
          <div className="mt-4 flex items-center gap-3 pb-3">
            <span className="text-sm font-medium text-muted-foreground">Interview Type:</span>
            <div className="flex gap-2">
              <Button
                variant={interviewType === 'personality' ? 'default' : 'outline'}
                size="sm"
                onClick={() => setInterviewType('personality')}
                className="text-xs h-8"
              >
                Personality Test
              </Button>
              <Button
                variant={interviewType === 'technical' ? 'default' : 'outline'}
                size="sm"
                onClick={() => setInterviewType('technical')}
                className="text-xs h-8"
              >
                Technical Interview
              </Button>
            </div>
          </div>
          
          <div className="mt-2">
            <Progress value={progress} className="h-2" />
          </div>
        </div>
      </header>

      {/* Interview Chat Area */}
      <div className="flex-1 overflow-hidden">
        <div className="container mx-auto px-4 py-6 h-full flex flex-col max-w-4xl">
          <div className="flex-1 overflow-y-auto space-y-4 mb-4">
            {messages.map((message, index) => (
              <div
                key={index}
                className={`flex gap-3 ${message.role === "user" ? "justify-end" : "justify-start"}`}
              >
                {message.role === "ai" && (
                  <Avatar className="size-10 border-2 border-primary">
                    <AvatarFallback className="bg-primary text-primary-foreground">AI</AvatarFallback>
                  </Avatar>
                )}
                <div
                  className={`max-w-[80%] rounded-2xl px-4 py-3 ${
                    message.role === "user"
                      ? "bg-primary text-primary-foreground"
                      : "bg-card border border-border"
                  }`}
                >
                  <p className="text-sm leading-relaxed whitespace-pre-wrap">{message.content}</p>
                  <span className="text-xs opacity-70 mt-2 block">
                    {message.timestamp.toLocaleTimeString([], { hour: '2-digit', minute: '2-digit' })}
                  </span>
                </div>
                {message.role === "user" && (
                  <Avatar className="size-10 border-2 border-accent">
                    <AvatarFallback className="bg-accent text-accent-foreground">You</AvatarFallback>
                  </Avatar>
                )}
              </div>
            ))}
            
            {isLoading && (
              <div className="flex gap-3 justify-start">
                <Avatar className="size-10 border-2 border-primary">
                  <AvatarFallback className="bg-primary text-primary-foreground">AI</AvatarFallback>
                </Avatar>
                <div className="bg-card border border-border rounded-2xl px-4 py-3">
                  <Loader2 className="size-4 animate-spin" />
                </div>
              </div>
            )}

            {interviewComplete && (
              <Card className="border-2 border-primary/20 bg-gradient-to-br from-primary/5 to-accent/5">
                <CardContent className="p-6 text-center space-y-4">
                  <div className="size-16 rounded-full bg-green-100 dark:bg-green-900/20 mx-auto flex items-center justify-center">
                    <CheckCircle className="size-8 text-green-600 dark:text-green-400" />
                  </div>
                  <div>
                    <h3 className="text-xl font-bold mb-2">Interview Complete!</h3>
                    <p className="text-muted-foreground">
                      Great job! Your responses have been analyzed and your feedback is ready.
                    </p>
                  </div>
                  <Button 
                    size="lg" 
                    className="gap-2"
                    onClick={() => router.push(`/feedback/${jobId}`)}
                  >
                    View Your Feedback
                    <ArrowRight className="size-4" />
                  </Button>
                </CardContent>
              </Card>
            )}
            
            <div ref={messagesEndRef} />
          </div>


          {/* Input Area */}
          {!interviewComplete && (
            <Card className="border-2">
              <CardContent className="p-4">
                <div className="flex gap-3">
                  <Textarea
                    placeholder={
                      interviewType === 'technical' 
                        ? "Í∏∞Ïà†Ï†Å ÏÑ∏Î∂ÄÏÇ¨Ìï≠Í≥º Íµ¨Ï≤¥Ï†ÅÏù∏ Íµ¨ÌòÑ Î∞©Î≤ïÏùÑ ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî... (Enter: Ï†ÑÏÜ°)"
                        : "Í≤ΩÌóòÍ≥º ÏÉÅÌô©ÏùÑ Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú ÎßêÏîÄÌï¥Ï£ºÏÑ∏Ïöî... (Enter: Ï†ÑÏÜ°)"
                    }
                    value={currentInput}
                    onChange={(e) => setCurrentInput(e.target.value)}
                    onKeyDown={handleKeyPress}
                    className="min-h-[60px] max-h-[200px] resize-none"
                    disabled={isLoading || isRecording}
                  />
                  
                  <div className="flex flex-col gap-2">
                    {/* Mic Button */}
                    <Button
                      size="icon"
                      variant={isRecording ? "destructive" : "outline"}
                      className="flex-shrink-0"
                      onClick={toggleRecording}
                      disabled={isLoading}
                      title={isRecording ? "Stop recording" : "Start voice input"}
                    >
                      {isRecording ? <MicOff className="size-4" /> : <Mic className="size-4" />}
                    </Button>
                    
                    {/* Send Button */}
                    <Button
                      size="icon"
                      className="flex-shrink-0"
                      onClick={() => handleSendMessage()}
                      disabled={!currentInput.trim() || isLoading || isRecording}
                    >
                      <Send className="size-4" />
                    </Button>
                  </div>
                </div>
                
                <div className="flex items-center justify-between mt-3 text-xs text-muted-foreground">
                  <div className="flex items-center gap-4">
                    {/* Speaker Toggle */}
                    <Button
                      variant={speakerEnabled ? "default" : "outline"}
                      size="sm"
                      className="h-7 px-3 gap-2"
                      onClick={() => setSpeakerEnabled(!speakerEnabled)}
                      title={speakerEnabled ? "Disable AI voice" : "Enable AI voice"}
                    >
                      {speakerEnabled ? <Volume2 className="size-3" /> : <VolumeX className="size-3" />}
                      <span className="text-xs">
                        {speakerEnabled ? "Voice ON" : "Voice OFF"}
                      </span>
                    </Button>
                    
                    {isRecording && (
                      <span className="flex items-center gap-2 text-red-500 animate-pulse">
                        <span className="size-2 rounded-full bg-red-500"></span>
                        Recording...
                      </span>
                    )}
                    
                    {isLoading && !isRecording && (
                      <span className="flex items-center gap-2 text-primary">
                        <Loader2 className="size-4 animate-spin" />
                        {isAISpeaking ? 'AIÍ∞Ä ÎãµÎ≥Ä Ï§ëÏûÖÎãàÎã§...' : 'AIÍ∞Ä ÎãµÎ≥ÄÏùÑ ÏÉùÏÑ± Ï§ëÏûÖÎãàÎã§...'}
                      </span>
                    )}
                  </div>
                  <span>EnterÎ•º ÎàåÎü¨ Ï†ÑÏÜ°</span>
                </div>
              </CardContent>
            </Card>
          )}
        </div>
      </div>
    </div>
  )
}
